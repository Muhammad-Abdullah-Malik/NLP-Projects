{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q datasets scikit-learn evaluate\n\n!pip install -U transformers\n\nfrom datasets import load_dataset\n\nfrom transformers import AutoTokenizer,AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport pandas as pd\nimport numpy as np\nimport evaluate\n\ndataset_name=\"ag_news\"\ndataset=load_dataset(dataset_name)\n\ntrain_dataset=dataset['train']\ntest_dataset=dataset['test']\n\ntest_dataset\n\nmodel_path=\"bert-base-uncased\"\nnum_labels=dataset['train'].features['label'].num_classes\nmodel=AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=num_labels)\nmodel.to(\"cuda\")\ntokenizer=AutoTokenizer.from_pretrained(model_path)\n\ntext=\"Hello how are you sir\"\ninputs=tokenizer(text,return_tensors=\"pt\")\n\nfor i,j in inputs.items():\n  print(f\"{i}:{j.shape}:\",j)\n\ndef preprocess_function(examples):\n  return tokenizer(examples[\"text\"],truncation=True,padding=\"max_length\")\n\ntrain_dataset=train_dataset.map(preprocess_function,batched=True)\ntest_dataset=test_dataset.map(preprocess_function,batched=True)\n\nfor param in model.base_model.parameters():\n  param.requires_grad=False\nfor param in model.bert.pooler.parameters():\n  param.requires_grad=True\n\n\n\n\n\n\nimport evaluate\nimport numpy as np\n\naccuracy=evaluate.load(\"accuracy\")\nprecision=evaluate.load(\"precision\")\nrecall=evaluate.load(\"recall\")\nf1=evaluate.load(\"f1\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=1)\n\n    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n\n    precision_score = precision.compute(\n        predictions=predictions,\n        references=labels,\n        average=\"macro\"\n    )\n\n    recall_score = recall.compute(\n        predictions=predictions,\n        references=labels,\n        average=\"macro\"\n    )\n\n    f1_score = f1.compute(\n        predictions=predictions,\n        references=labels,\n        average=\"macro\"\n    )\n\n    return {\n        \"accuracy\": accuracy_score,\n        \"precision\": precision_score,\n        \"recall\": recall_score,\n        \"f1\": f1_score\n    }\n\n\ntraining_args=TrainingArguments(\n    output_dir=\"my_model\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    load_best_model_at_end=True,\n    logging_dir=\"logs\",\n    logging_steps=10,\n    log_level=\"info\",\n    report_to=\"none\",\n    metric_for_best_model=\"accuracy\"\n)\n\ntrainer=Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    compute_metrics=compute_metrics,\n    processing_class=tokenizer\n)\n\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T05:07:43.261102Z","iopub.execute_input":"2025-11-30T05:07:43.261259Z","iopub.status.idle":"2025-11-30T06:34:49.823272Z","shell.execute_reply.started":"2025-11-30T05:07:43.261244Z","shell.execute_reply":"2025-11-30T06:34:49.822337Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting transformers\n  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.10.5)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\nSuccessfully installed tokenizers-0.22.1 transformers-4.57.3\n","output_type":"stream"},{"name":"stderr","text":"2025-11-30 05:08:18.446467: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764479298.656437      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764479298.718288      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"918f5a337b3144b7b4ef4e5e90cd7c74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"378bfa90f3db4b19985dea651eae83a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d27406767914219af114046def9b4c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76d45e47e74f4f93bd65db2e68a2379b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c50722e8e060491a89284bde6ee34608"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2a5ca5415cc4762853610009bdc6542"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba94a910016b4af195f989eaa2f2c7c5"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358ec330893843d693db3e5a5e845423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed78cc7beea7492a88997d295b22e411"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc1d69313dee4bd4b6d980b069ffad45"}},"metadata":{}},{"name":"stdout","text":"input_ids:torch.Size([1, 7]): tensor([[ 101, 7592, 2129, 2024, 2017, 2909,  102]])\ntoken_type_ids:torch.Size([1, 7]): tensor([[0, 0, 0, 0, 0, 0, 0]])\nattention_mask:torch.Size([1, 7]): tensor([[1, 1, 1, 1, 1, 1, 1]])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a174c90937e49a3bca4a8095b245743"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b838a44f4da4cd387ea71735aca99d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f42100b5bf441fbc301153e2d25e9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3af84212661460eac9e3dc00a30a0fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e35cf078f795448ab6b228e7d6026ac7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4b2d79f283d4723b732a32210320580"}},"metadata":{}},{"name":"stderr","text":"The following columns in the Training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 120,000\n  Num Epochs = 2\n  Instantaneous batch size per device = 16\n  Training with DataParallel so batch size has been adjusted to: 32\n  Total train batch size (w. parallel, distributed & accumulation) = 32\n  Gradient Accumulation steps = 1\n  Total optimization steps = 7,500\n  Number of trainable parameters = 593,668\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7500/7500 1:25:14, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.315000</td>\n      <td>0.322067</td>\n      <td>0.885395</td>\n      <td>{'precision': 0.8854887393282387}</td>\n      <td>{'recall': 0.8853947368421051}</td>\n      <td>{'f1': 0.8854379804490082}</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.345800</td>\n      <td>0.311975</td>\n      <td>0.890658</td>\n      <td>{'precision': 0.8906587530602841}</td>\n      <td>{'recall': 0.8906578947368421}</td>\n      <td>{'f1': 0.8906517741836247}</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 7600\n  Batch size = 32\nSaving model checkpoint to my_model/checkpoint-3750\nConfiguration saved in my_model/checkpoint-3750/config.json\nModel weights saved in my_model/checkpoint-3750/model.safetensors\ntokenizer config file saved in my_model/checkpoint-3750/tokenizer_config.json\nSpecial tokens file saved in my_model/checkpoint-3750/special_tokens_map.json\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThe following columns in the Evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 7600\n  Batch size = 32\nSaving model checkpoint to my_model/checkpoint-7500\nConfiguration saved in my_model/checkpoint-7500/config.json\nModel weights saved in my_model/checkpoint-7500/model.safetensors\ntokenizer config file saved in my_model/checkpoint-7500/tokenizer_config.json\nSpecial tokens file saved in my_model/checkpoint-7500/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nLoading best model from my_model/checkpoint-7500 (score: 0.8906578947368421).\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7500, training_loss=0.3635044925689697, metrics={'train_runtime': 5116.4929, 'train_samples_per_second': 46.907, 'train_steps_per_second': 1.466, 'total_flos': 6.314778722304e+16, 'train_loss': 0.3635044925689697, 'epoch': 2.0})"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"labelmap=dataset['train'].features['label'].int2str\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:47:46.960104Z","iopub.execute_input":"2025-11-30T06:47:46.960399Z","iopub.status.idle":"2025-11-30T06:47:46.965249Z","shell.execute_reply.started":"2025-11-30T06:47:46.960378Z","shell.execute_reply":"2025-11-30T06:47:46.964708Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'Sports'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch\ndef classify_text(text):\n    inputs=tokenizer(text,truncation=True,padding=True,max_length=256,return_tensors=\"pt\")\n    inputs={k:v.to(model.device) for k,v in inputs.items()}\n    with torch.no_grad():\n        outputs=model(**inputs)\n    predicted_id=outputs.logits.argmax(dim=-1).item()\n    return labelmap(predicted_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:58:02.746442Z","iopub.execute_input":"2025-11-30T06:58:02.747185Z","iopub.status.idle":"2025-11-30T06:58:02.752561Z","shell.execute_reply.started":"2025-11-30T06:58:02.747151Z","shell.execute_reply":"2025-11-30T06:58:02.751894Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"print(classify_text(\"Pakistan is the land famous of its tourism\"))\nprint(classify_text(\"Marvel has released a new movie\"))\nprint(classify_text(\"Babarazam is the best batsman Pakistan has ever produced\"))\nprint(classify_text(\"Pakistan is the land famous of its tourism\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:01:07.160402Z","iopub.execute_input":"2025-11-30T07:01:07.160668Z","iopub.status.idle":"2025-11-30T07:01:07.203448Z","shell.execute_reply.started":"2025-11-30T07:01:07.160648Z","shell.execute_reply":"2025-11-30T07:01:07.202723Z"}},"outputs":[{"name":"stdout","text":"World\nSci/Tech\nSports\nWorld\n","output_type":"stream"}],"execution_count":45}]}